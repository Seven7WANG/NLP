{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "mount_file_id": "1-_Vw5jMSI2nu7fLIqHjvbUp25ra6KyTl",
      "authorship_tag": "ABX9TyPqFP9vuTj7tHVwtynBSl/X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seven7WANG/NLP/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Zp3HaQPmQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import spacy \n",
        "import sklearn.decomposition as PCA\n",
        "from gensim.models import word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6K19US1Qz3j",
        "colab_type": "text"
      },
      "source": [
        "Input and Output\n",
        "Input: Word embeddings {$ v_w: w\\in V$}, a set of sentences $S$, parameter $a$ and estimated probabilities {$ p(w): w\\in V$} of the words  \n",
        "Word embeddings: Glove or word2vec(CBOW)  \n",
        "Output: Sentence embeddings {$ v_s: s\\in S$}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzn3mZY3Q-7T",
        "colab_type": "text"
      },
      "source": [
        "First, train word embeddings for words, using word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP3gJjcUQvWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b9f861ef-b541-498e-bcdf-db7de7d8c884"
      },
      "source": [
        "from gensim.test.utils import common_texts, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import logging\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "path = get_tmpfile(\"word2vec.model\")\n",
        "\n",
        "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec.model\")"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULQUwdEsQ5OL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "9891b577-f157-4838-999f-c772d8489630"
      },
      "source": [
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)\n",
        "vector = model.wv['computer']\n",
        "print(vector)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.5436478e-03 -4.2476543e-04 -3.5180766e-03 -1.8783556e-03\n",
            "  3.6550774e-03  6.8219152e-04  2.5455621e-03  3.1596238e-03\n",
            " -2.6719521e-03  1.0142430e-03  4.1363342e-03 -4.1144881e-03\n",
            " -4.8565208e-03  4.0023960e-03 -1.4083482e-03 -2.2731607e-03\n",
            "  4.8687076e-03  1.5194154e-03 -1.8947924e-03 -6.3783827e-04\n",
            "  4.5433282e-03  4.4369134e-03  4.1188477e-03  3.6772275e-03\n",
            "  4.9795890e-03 -4.2066062e-03 -9.8388933e-04 -8.2404522e-04\n",
            " -2.4195726e-03 -6.6597533e-04 -1.1459017e-03  2.9498469e-03\n",
            " -1.8169238e-03  2.0748687e-03 -4.5803045e-03 -2.3495376e-03\n",
            "  4.9705748e-03  4.3151653e-03  3.7588861e-03  3.6195465e-03\n",
            " -3.4232258e-03 -2.7712181e-03 -1.6443064e-03  2.7288150e-03\n",
            " -4.9770358e-03  2.5547862e-03 -2.9565881e-03  4.7236239e-03\n",
            "  5.3143426e-04  7.9043282e-05 -4.4887383e-03 -6.4106553e-04\n",
            " -2.3658630e-03  1.0506761e-03  2.2398958e-03 -4.2802622e-03\n",
            " -2.7821643e-05  4.3426231e-03 -3.0745713e-03  8.4879837e-04\n",
            "  3.9576134e-03 -4.5315796e-04 -1.9357199e-04  4.0604579e-03\n",
            "  3.6254069e-03  2.8963403e-03 -2.8207274e-03  2.1724082e-03\n",
            " -4.2471229e-03  2.7485320e-03 -3.4129922e-03 -1.1814536e-03\n",
            " -1.2823591e-03 -4.7731460e-03  4.4467151e-03 -8.3643856e-04\n",
            "  4.7132983e-03  8.1131340e-04 -1.8985622e-03 -7.8050297e-04\n",
            "  2.8035589e-03  2.6158635e-03  3.0363833e-03 -9.6049678e-04\n",
            " -7.8704418e-04 -3.1828615e-04 -4.9733282e-03  4.1149301e-03\n",
            "  9.8473998e-04  3.0916068e-03  1.9506245e-03  1.9937349e-03\n",
            "  6.3381798e-04  1.3439630e-03 -4.7517461e-03 -2.9171966e-03\n",
            " -1.9166883e-03  2.7679892e-03 -4.8790998e-03  3.6569356e-05]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKjAy-LQfPUc",
        "colab_type": "text"
      },
      "source": [
        "Dataset:\n",
        "Semantic Textual Similarity (STS), 2014  \n",
        "Word embedding: Glove, Word2Vec,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nhimrPHiBm9",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_lke0C6kc6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkTEhK9HaXKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_processing(path):\n",
        "  # sentences = []\n",
        "  with open(path) as reader:\n",
        "    for line in reader:\n",
        "      # obtain tokens\n",
        "      line1, line2 = line.split('\\t', 1)\n",
        "      tokens = re.sub(r\"[^a-z0-9]+\", \" \", line1.lower()).split()\n",
        "      sentences.append(tuple(tokens))\n",
        "      tokens = re.sub(r\"[^a-z0-9]+\", \" \", line2.lower()).split()\n",
        "      sentences.append(tuple(tokens))\n",
        "      # yield gensim.utils.simple_process(line)\n",
        "  # print(sentences)\n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ApPcWYfaldN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentences = []\n",
        "# path = '/content/drive/My Drive/Sentence2Vec/example.txt'\n",
        "# word_processing(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqlDAwy6ji5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "a3ade593-1dfd-4e48-c16c-4694897ed16f"
      },
      "source": [
        "sentences = []\n",
        "path1 = '/content/drive/My Drive/Sentence2Vec/STS.input.OnWN.txt'\n",
        "path2 = '/content/drive/My Drive/Sentence2Vec/STS.input.deft-forum.txt'\n",
        "path3 = '/content/drive/My Drive/Sentence2Vec/STS.input.deft-news.txt'\n",
        "path4 = '/content/drive/My Drive/Sentence2Vec/STS.input.headlines.txt'\n",
        "path5 = '/content/drive/My Drive/Sentence2Vec/STS.input.images.txt'\n",
        "path6 = '/content/drive/My Drive/Sentence2Vec/STS.input.tweet-news.txt'\n",
        "# token = tuple(word_processing(path1))\n",
        "# sentences.append(token)\n",
        "# sentences = tuple(sentences)\n",
        "word_processing(path1)\n",
        "# print(tuple(sentences))\n",
        "print(len(sentences))\n",
        "word_processing(path2)\n",
        "print(len(sentences))\n",
        "word_processing(path3)\n",
        "print(len(sentences))\n",
        "word_processing(path4)\n",
        "print(len(sentences))\n",
        "word_processing(path5)\n",
        "print(len(sentences))\n",
        "word_processing(path6)\n",
        "print(len(sentences))\n",
        "print(sentences[:2])"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500\n",
            "2400\n",
            "3000\n",
            "4500\n",
            "6000\n",
            "7500\n",
            "[('the', 'activity', 'of', 'learning', 'or', 'being', 'trained'), ('the', 'gradual', 'process', 'of', 'acquiring', 'knowledge')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gDJYs8SoWmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: use words to train Word2Vec model\n",
        "# print(sentences)\n",
        "model = Word2Vec(\n",
        "    sentences,\n",
        "    size=100, # dimensionality of the word vectors\n",
        "    window=10, # window size\n",
        "    min_count=1,\n",
        "    workers=4,\n",
        "    iter=10\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "251jHlRqv111",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "cdf22ff2-37ca-49a9-fa7b-020d85465b4d"
      },
      "source": [
        "print(model.wv.most_similar(\"seven\", topn=10))"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('palestinians', 0.9996417164802551), ('50', 0.9996374845504761), ('protesters', 0.9996190071105957), ('11', 0.9995981454849243), ('pakistani', 0.9995980262756348), ('13', 0.9995725750923157), ('northern', 0.9995689392089844), ('bombing', 0.9995410442352295), ('israeli', 0.9994544386863708), ('helicopter', 0.9994449019432068)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1X4xPNVpKNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Todo: define Word and Sentence structure\n",
        "class Word:\n",
        "  def __init__(self, word, vector):\n",
        "    self.word = word\n",
        "    self.vector = vector\n",
        "\n",
        "class Sentence:\n",
        "  def __init__(self, words):\n",
        "    self.word_list = words\n",
        "    self.length = len(words)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g621nUy6pc61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Todo: obtain word vectors, sentences\n",
        "# sentences: sentence_list\n",
        "def store_vectors(sentences):\n",
        "  sentence_list = []\n",
        "  # print(sentences[0])\n",
        "  word2vec_model = model.wv\n",
        "  for sentence in sentences:\n",
        "    word_list = []\n",
        "    for word in sentence:\n",
        "      if word in word2vec_model.vocab:\n",
        "        embed = word2vec_model.get_vector(word)\n",
        "        word_list.append(Word(word, embed))\n",
        "      # if embed is not None:\n",
        "        \n",
        "    if len(word_list) > 0:\n",
        "      sentence_list.append(Sentence(word_list))\n",
        "        # print(word, embed)\n",
        "  return sentence_list\n",
        "\n",
        "# sentence_list = store_vectors(sentences)\n",
        "# print(sentence_list[0].word_list[0].word)\n",
        "# print(sentence_list[0].word_list[0].vector)\n",
        "# print(sentence_list[0].length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIQY1dcsgew-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define some stop words\n",
        "stop_words = {'ourselves','hers','between','yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'us', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmrwh-5NRhqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5c112e3e-19da-4506-e431-3f124ae7ea3d"
      },
      "source": [
        "# Todo: read csv file, and organize word frequency\n",
        "# word_freq: dictionary, (word: times); total: sum of times of all words\n",
        "# total excludes the number of stop_words --> stop words: frequency bigger\n",
        "def read_word_frequency(path):\n",
        "  # words = pd.read_csv(path, encoding = 'utf-8', header=None, low_memory=False)\n",
        "  word_freq = pd.read_csv(path, index_col=0, squeeze=True, header=None, low_memory=False).to_dict()\n",
        "  print(word_freq.get('a'))\n",
        "  # print(word_freq.get('password'))\n",
        "  total = 0\n",
        "  stop = []\n",
        "  for i in list(word_freq):\n",
        "    if i in stop_words:\n",
        "      stop.append(i)\n",
        "    else:\n",
        "      # word_freq.update({i: })\n",
        "      total += int(word_freq.get(i))\n",
        "  # print(total)\n",
        "  # del d[key for key in stop]\n",
        "  return word_freq, total\n",
        "\n",
        "word_freq = dict()\n",
        "path = '/content/drive/My Drive/Sentence2Vec/unigram_freq.csv'\n",
        "word_freq, total = read_word_frequency(path)\n",
        "total = float(total)\n",
        "print(word_freq.get('goofel'))\n",
        "print(total)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9081174698\n",
            "12711\n",
            "401178185585.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiqIIleUpN1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42ed2c2c-70be-4615-d8a7-d109e7e9867a"
      },
      "source": [
        "# Todo: get word frequency from commencrawl dataset\n",
        "# use dataset: English Word Frequency\n",
        "def word_frequency(word):\n",
        "  if word in stop_words:\n",
        "    return 0.005\n",
        "  elif word not in word_freq.keys():\n",
        "    return 0.00001\n",
        "  elif float(word_freq.get(word)) < 6000000:\n",
        "    return 0.00001\n",
        "  else:\n",
        "    return round(float(word_freq.get(word))/total, 5)\n",
        "print(word_frequency('new'))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwDggXBzpfIQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb5b33cb-1b5d-4b76-f22d-a1383585a022"
      },
      "source": [
        "# Todo: define sentence2vec function\n",
        "# a should be fine tuned\n",
        "# sentence_vector: vs in Algorithm 1 in paper\n",
        "def Sentence2Vec(sentences, embedding_size, a):\n",
        "  sentence_list = []\n",
        "  print(sentences[0])\n",
        "  for sentence in sentences:\n",
        "    word_vec = np.zeros(embedding_size)    # padding\n",
        "    wordsNum = sentence.length\n",
        "    for word in sentence.word_list:\n",
        "      # print(word.word)\n",
        "      first_part = a / (a + word_frequency(word.word))\n",
        "      whole = first_part * word.vector\n",
        "      word_vec = np.add(word_vec, whole)\n",
        "    word_vec = np.divide(word_vec, wordsNum)\n",
        "    sentence_list.append(word_vec)\n",
        "  return sentence_list\n",
        "\n",
        "sentence_vector = Sentence2Vec(sentence_list, 100, 1e-3)\n",
        "# print(sentence_vector[:3])"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Sentence object at 0x7fe2712c0b38>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mAFsX1_o208",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Todo: calculate euclidean distance between two vectors\n",
        "def euclidean_distance(v1, v2):\n",
        "  total = 0.0\n",
        "  for i in range(len(v1)):\n",
        "    total += (v1[i] - v2[i]) ** 2\n",
        "  return math.sqrt(total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ5VzlKCo5yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1f9fe943-f429-45bc-d64a-60d585bd7dbc"
      },
      "source": [
        "# Todo: apply single sentence word embedding\n",
        "print(sentences[1100], sentences[1101])\n",
        "print(euclidean_distance(sentence_vector[1100], sentence_vector[1101]))\n",
        "neg = sentences[102]\n",
        "print(neg)\n",
        "neg_vector = sentence_vector[102]\n",
        "print(euclidean_distance(sentence_vector[1100], neg_vector))\n",
        "print(euclidean_distance(sentence_vector[1101], neg_vector))"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('cause', 'to', 'run', 'at', 'a', 'moderately', 'swift', 'pace') ('cause', 'bodily', 'suffering', 'to', 'and', 'make', 'sick', 'or', 'indisposed')\n",
            "0.15289959894588023\n",
            "('operating', 'or', 'traveling', 'via', 'a', 'vehicle')\n",
            "0.4426872176787265\n",
            "0.40064177756280844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E5Nr9s0BinM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_test(path):\n",
        "  with open(path) as reader:\n",
        "    for line in reader:\n",
        "      # obtain tokens\n",
        "      line1, line2 = line.split('\\t', 1)\n",
        "      tokens = re.sub(r\"[^a-z0-9]+\", \" \", line1.lower()).split()\n",
        "      test.append(tuple(tokens))\n",
        "      tokens = re.sub(r\"[^a-z0-9]+\", \" \", line2.lower()).split()\n",
        "      test.append(tuple(tokens))\n",
        "  return test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW8NzldKo93Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "206d926e-cf58-48ee-f61d-2f58bab5ea7c"
      },
      "source": [
        "# show the result\n",
        "path = '/content/drive/My Drive/Sentence2Vec/STS.test.headlines.txt'\n",
        "test = []\n",
        "test = read_test(path)\n",
        "test_list = store_vectors(test)\n",
        "test_vector = Sentence2Vec(test_list, 100, 1e-3)\n",
        "length = len(test_vector) // 2\n",
        "for i in range(length):\n",
        "  print(test[2*i], test[2*i+1])\n",
        "  print(euclidean_distance(test_vector[i], test_vector[i+1]))"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.Sentence object at 0x7fe2744c2ef0>\n",
            "('fourth', 'arrest', 'in', 'body', 'in', 'bin', 'probe') ('three', 'held', 'after', 'body', 'found', 'in', 'bin')\n",
            "0.3493268283940001\n",
            "('indian', 'air', 'force', 'to', 'buy', '126', 'rafale', 'fighter', 'jets') ('india', 'awards', 'dassault', '12', 'billion', 'contract', 'to', 'replenish', 'ageing', 'air', 'force')\n",
            "0.20977687019286057\n",
            "('bbc', 'news', 'deaths', 'in', 'ukraine', 'and', 'poland', 'in', 'freezing', 'europe', 'weather') ('ukraine', 'europe', 'hit', 'by', 'deadly', 'cold', 'snap')\n",
            "0.1815698878079397\n",
            "('gingrich', 'takes', 'georgia', 'leaning', 'on', 'a', 'southern', 'based', 'strategy') ('super', 'tuesday', 'newt', 'gingrich', 'says', 'he', 's', 'a', 'survivor')\n",
            "0.6538932448813514\n",
            "('iranian', 'hopes', 'dashed', 'at', 'non', 'aligned', 'summit') ('un', 'chief', 'asks', 'iran', 'to', 'prove', 'its', 'nuclear', 'program', 'peaceful')\n",
            "0.468062571672637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qNehRrcpGvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}